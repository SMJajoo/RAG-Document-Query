{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9cd1ea1-afdd-4d8a-b314-8b018de43c84",
   "metadata": {},
   "source": [
    "# Load and extract text from PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a83507-fa41-4795-8d79-96643894f889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting pdfplumber==0.11.1\n",
      "  Downloading pdfplumber-0.11.1-py3-none-any.whl (57 kB)\n",
      "Collecting pdfminer.six==20231228\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\sanskruti jajoo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pdfplumber==0.11.1) (10.4.0)\n",
      "Collecting pypdfium2>=4.18.0\n",
      "  Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)\n",
      "Collecting charset-normalizer>=2.0.0\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl (100 kB)\n",
      "Collecting cryptography>=36.0.0\n",
      "  Downloading cryptography-43.0.0-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "Collecting cffi>=1.12\n",
      "  Downloading cffi-1.16.0-cp39-cp39-win_amd64.whl (181 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: pycparser, cffi, cryptography, charset-normalizer, pypdfium2, pdfminer.six, pdfplumber\n",
      "Successfully installed cffi-1.16.0 charset-normalizer-3.3.2 cryptography-43.0.0 pdfminer.six-20231228 pdfplumber-0.11.1 pycparser-2.22 pypdfium2-4.30.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script normalizer.exe is installed in 'c:\\Users\\Sanskruti Jajoo\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pypdfium2.exe is installed in 'c:\\Users\\Sanskruti Jajoo\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pdfplumber.exe is installed in 'c:\\Users\\Sanskruti Jajoo\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 21.1.1; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Sanskruti Jajoo\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install pdfplumber==0.11.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a03c2ab-2a18-4a7d-b0e4-0e0243813e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467aba5a-1f5c-4b87-8929-cb0c4ef7677e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NexGen AI Tech Solutions\n",
      "Quarterly Earnings\n",
      "Report for Q2 2024\n",
      "4th June, 2024 Contents\n",
      "1. ExecutiveSummary\n",
      "2. FinancialPerformance\n",
      "3. RevenuebyDepartment\n",
      "4. StrategicInitiatives\n",
      "5. ExpectedPerformancefortheRestof2024\n",
      "6. Conclusion\n",
      "1 Executive Summary\n",
      "InQ22024,NexGenAITechSolutionscontinuedtodriveexceptionalgrowththroughthe\n",
      "strategicintegrationofAItechnologiesacrossourservicespectrum.Theadoptionof\n",
      "cutting-edgeAIapplicationshasresultedinaremarkable20%growthintotalrevenuecompared\n",
      "toQ22023,affirmingourleadershipinthetechsolutionssector.\n",
      "2 Financial Performance\n",
      "FinancialPerformanceinQ22024\n",
      "● TotalRevenue:$6.5million,up20%from$5.4millioninQ22023.\n",
      "● GrossProfit:$4.8million,representingagrossmarginof73.8%.\n",
      "● OperatingExpenses:$2.5million,focusedonexpandingourAIcapabilitiesand\n",
      "infrastructure.\n",
      "● NetIncome:$2.3million,anetmarginof35.4%,upfrom$2.0millioninQ22023.\n",
      "3 Revenue by Department\n",
      "● AI-PoweredCloudServices:$2.8million,up25%from$2.24million.\n",
      "● AI-EnhancedCybersecuritySolutions:$2.0million,up18%from$1.7million.\n",
      "● AIStrategiesConsulting:$1.2million,up10%from$1.09million.\n",
      "● SupportandAIMaintenance:$500,000,consistentgrowthat10%from$454,000.\n",
      "4 Strategic Initiatives\n",
      "StrategicInitiativesfor2024\n",
      "Fortheremainderof2024,NexGenAITechSolutionsaimsto:\n",
      "1. ExpandourAIresearchanddevelopmenttopioneernewtechsolutions.\n",
      "2. StrengthenourmarketpresenceinEuropeandAsia.\n",
      "3. EnhancecustomerengagementthroughAI-drivenanalyticsandpersonalizedservices.\n",
      "1.ExpandAIResearchandDevelopment\n",
      "NexGenAITechSolutionsplanstoincreaseinvestmentinourAIresearchanddevelopment\n",
      "departmentby20%overthenexttwoquarters.Thisexpansionwillfocusondeveloping\n",
      "proprietaryAIalgorithmsthatcanprovidepredictiveanalyticsforcloudcomputingandenhance\n",
      "threatdetectionincybersecuritysolutions.Byleadinginnovationintheseareas,weaimtoset\n",
      "newindustrystandardsandprovideourclientswithunmatchedtechnologicaladvantages.\n",
      "2.StrengthenMarketPresenceinEuropeandAsia\n",
      "InresponsetogrowingdemandforAIsolutionsinEuropeandAsia,NexGenAITechSolutions\n",
      "willinitiateaseriesofmarketentryandexpansionstrategies.Theseincludeestablishing\n",
      "partnershipswithlocalfirms,participatinginmajortechexpos,andsettingupregionalofficesin\n",
      "keycitiessuchasBerlin,Singapore,andTokyo.Theseeffortsareexpectedtoincreaseour\n",
      "internationalsalesby15%andenhanceglobalbrandrecognition.\n",
      "3.EnhanceCustomerEngagementThroughAI-DrivenAnalytics\n",
      "WeplantolaunchanewcustomerengagementplatformthatutilizesAItopersonalize\n",
      "interactionsandimproveservicedelivery.Thisplatformwillanalyzecustomerdatainreal-timeto\n",
      "offercustomizedrecommendationsandsupport.Enhancementsincustomerserviceare\n",
      "projectedtoincreasecustomersatisfactionratesby10%andreducechurnby5%.\n",
      "5 Expected Performance for the Rest of 2024\n",
      "Basedonthestrategicinitiativesoutlinedandcurrentmarkettrends,NexGenAITechSolutionsis\n",
      "positionedforstrongperformancethroughouttheremainderof2024.Weanticipatethefollowing\n",
      "developments:\n",
      "RevenueGrowth:ContinuedinvestmentinAIcapabilitiesandexpansionintonewmarketsare\n",
      "expectedtodrivearevenueincreaseofapproximately12-15%bytheendoftheyear.\n",
      "ProfitMargins:AsourAIsolutionsgainmaturityandefficiency,weexpecttoseeimprovedprofit\n",
      "margins,potentiallyreachinganetmarginof37%bytheyear'send.\n",
      "MarketShare:Withaggressiveexpansionandinnovativeofferings,ourmarketshareintheAI\n",
      "solutionssectorisprojectedtogrowby8%globally.\n",
      "CustomerBaseExpansion:Initiativesaimedatenhancingcustomerengagementandentering\n",
      "newmarketsarelikelytoexpandourcustomerbaseby20%,particularlyintheenterpriseand\n",
      "mid-marketsegments.\n",
      "Theseprojectionsarebasedoncurrentstrategicplansandmarketconditions,andtheyhighlight\n",
      "ouroptimisticoutlookforthefinancialandoperationalgrowthofNexGenAITechSolutionsin\n",
      "2024.Ifyouneedfurtherdetailsoradjustmentstothisforecast,pleaseletmeknow!\n",
      "6 Conclusion\n",
      "NexGenAITechSolutionsisonasolidpathofgrowth,withAIintegrationprovingtobeakey\n",
      "driverofoursuccess.Asweadvancethrough2024,weremaincommittedtoleveragingAIto\n",
      "deliversuperiortechnologysolutions,ensuringrobustfinancialperformanceandcontinued\n",
      "marketleadership.\n",
      "7 NexGen AI Tech Solutions - Quarterly Earnings Meeting - Q2 2024\n",
      "Quarterly Earnings Meeting - Q2 2024\n",
      "Date: June 15, 2024\n",
      "Location: Conference Room A\n",
      "Attendees:\n",
      "- Jessica Hynes, CEO\n",
      "- Martin Wells, CFO\n",
      "- Samantha Lee, CTO\n",
      "- Alan Rickman, Director of AI Research and Development\n",
      "- Tanya Morris, Director of European Market Expansion\n",
      "- Henry Cho, Director of Asian Market Operations\n",
      "- Elaine Gupta, Customer Engagement Manager\n",
      "Meeting Agenda:\n",
      "1. Review of Q2 Financial Performance\n",
      "2. Departmental Revenue Analysis\n",
      "3. Strategic Initiatives and Roadmap for 2024\n",
      "4. Customer Engagement Enhancements\n",
      "5. Market Trends and Projections\n",
      "6. Open Discussion and Strategic Adjustments\n",
      "7. Conclusion and Next Steps\n",
      "Detailed Meeting Notes:\n",
      "Page 1 NexGen AI Tech Solutions - Quarterly Earnings Meeting - Q2 2024\n",
      "1. Financial Overview:\n",
      "Martin Wells reported a significant 20% increase in total revenue for Q2 2024, reaching $6.5\n",
      "million. The net income rose to $2.3 million, marking a robust growth from the previous year.\n",
      "2. Revenue by Department:\n",
      "Samantha Lee highlighted exceptional growth in AI-Powered Cloud Services and AI-Enhanced\n",
      "Cybersecurity Solutions. A consistent growth was maintained in Support and AI Maintenance\n",
      "services.\n",
      "3. Strategic Initiatives for 2024:\n",
      "Alan Rickman detailed the plan to enhance AI R&D, with a budget increase of 20% aimed at\n",
      "developing proprietary AI algorithms.\n",
      "Tanya Morris and Henry Cho discussed strategies for strengthening market presence in Europe\n",
      "and Asia.\n",
      "4. Customer Engagement Enhancements:\n",
      "Elaine Gupta introduced a cutting-edge AI-driven platform designed to personalize customer\n",
      "interactions.\n",
      "5. Market Trends and Projections:\n",
      "Based on the strategic initiatives and current market dynamics, Henry Cho projected a 12-15%\n",
      "revenue increase by the year-end.\n",
      "6. Open Discussion:\n",
      "Page 2 NexGen AI Tech Solutions - Quarterly Earnings Meeting - Q2 2024\n",
      "Discussions revolved around potential challenges in international expansions, such as regulatory\n",
      "hurdles and local competition.\n",
      "7. Conclusion:\n",
      "Jessica Hynes concluded the meeting by appreciating the team's efforts and setting a follow-up\n",
      "meeting to review the progress on the discussed initiatives.\n",
      "Page 3 \n"
     ]
    }
   ],
   "source": [
    "combined_text = ''\n",
    "files_directory = 'files'\n",
    " \n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(files_directory):\n",
    "    if filename.endswith('.pdf'):\n",
    "        # Open the PDF file\n",
    "        with pdfplumber.open(os.path.join(files_directory, filename)) as pdf:\n",
    "            # Loop through all pages in the PDF file\n",
    "            for page in pdf.pages:\n",
    "                # Extract the text from the page and add it to the rest of the text\n",
    "                combined_text += page.extract_text() + ' '\n",
    "                \n",
    "print(combined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d225f04-a8fc-4ec5-a7ff-23de5d2b6fe6",
   "metadata": {},
   "source": [
    "# Text Preprocessign and Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbd8bef-a578-4b29-b218-97c27501eaef",
   "metadata": {},
   "source": [
    "By using a text splitter the goal is to optimize large text handling, enhancing LLM performance and processing efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "241b3706-bba4-47cb-822a-e427c187df2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6156\n"
     ]
    }
   ],
   "source": [
    "print(len(combined_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e80b13-5fbd-486e-8dfb-76cf6d30e8f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mBad file descriptor (C:\\Users\\runneradmin\\AppData\\Local\\Temp\\tmpkywtyyea\\build\\_deps\\bundled_libzmq-src\\src\\epoll.cpp:73). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers==3.0.1\n",
    "%pip install langchain-community==0.2.5\n",
    "%pip install langchain-huggingface==0.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "989c774f-c4aa-472a-a09a-9394668665de",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m      3\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m text_chunks \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_text(combined_text)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "text_chunks = text_splitter.split_text(combined_text)\n",
    "\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a2962-a1fa-401b-944e-4f39acbc1698",
   "metadata": {},
   "source": [
    "# Generate Text Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e13a6-4761-49c5-8242-8d46aeac986b",
   "metadata": {},
   "source": [
    "To allow for more accurate and relevant search results, represent the text in the PDF documents as vectors by creating embeddings, which are numerical representations of text data.\n",
    "\n",
    "Use an open-source sentence transformer model from HuggingFace to compute the embeddings: \n",
    "\n",
    "sentence-transformers/paraphrase-MiniLM-L6-v2\n",
    "\n",
    "Store the texts and the embeddings in a FAISS (Facebook AI Similarity Search) vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063217d-2e82-43a2-93d9-d8f851298562",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mBad file descriptor (C:\\Users\\runneradmin\\AppData\\Local\\Temp\\tmpkywtyyea\\build\\_deps\\bundled_libzmq-src\\src\\epoll.cpp:73). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831afef-b813-4a4a-a18a-83f4eac8f288",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mBad file descriptor (C:\\Users\\runneradmin\\AppData\\Local\\Temp\\tmpkywtyyea\\build\\_deps\\bundled_libzmq-src\\src\\epoll.cpp:73). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu==1.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a752a20-e5cb-4078-81b9-106a8b50dda1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mBad file descriptor (C:\\Users\\runneradmin\\AppData\\Local\\Temp\\tmpkywtyyea\\build\\_deps\\bundled_libzmq-src\\src\\epoll.cpp:73). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_texts(text_chunks, embeddings)\n",
    "print(db.index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a94fa48-076e-49cb-9c4d-53c6946f2c35",
   "metadata": {},
   "source": [
    "# Setup the Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da31a0fd-d08b-4985-a262-5f6c9fea7904",
   "metadata": {},
   "source": [
    "Convert the FAISS vector store into a retriever that can return documents for a given unstructured query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839a0e7-9568-4aaf-8231-8fac33e1a8fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mBad file descriptor (C:\\Users\\runneradmin\\AppData\\Local\\Temp\\tmpkywtyyea\\build\\_deps\\bundled_libzmq-src\\src\\epoll.cpp:73). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780cb2d-e8fd-479d-84ce-9b8c9febadfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mBad file descriptor (C:\\Users\\runneradmin\\AppData\\Local\\Temp\\tmpkywtyyea\\build\\_deps\\bundled_libzmq-src\\src\\epoll.cpp:73). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(retriever.invoke(\"Who is the CEO?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60edc3de-5e74-4426-b372-a812744fc5af",
   "metadata": {},
   "source": [
    "# Create a RAG prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd58bbc-65dc-4243-b5be-f57a227925d2",
   "metadata": {},
   "source": [
    "Create an appropriate prompt template that includes both the question and the necessary context to answer the question posted by the user.\n",
    "\n",
    "The goal of defining a prompt template is to translate user input and parameters into clear instructions for the OpenAI language model.  This will help the model understand the context and answer the question posted by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17488196-c29c-42e4-adfd-2613ddd842cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mBad file descriptor (C:\\Users\\runneradmin\\AppData\\Local\\Temp\\tmpkywtyyea\\build\\_deps\\bundled_libzmq-src\\src\\epoll.cpp:73). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    " \n",
    "    Question: {input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc086d7-b07f-49c3-a547-8c6422418063",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mBad file descriptor (C:\\Users\\runneradmin\\AppData\\Local\\Temp\\tmpkywtyyea\\build\\_deps\\bundled_libzmq-src\\src\\epoll.cpp:73). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f32d5ac-ef44-41d4-b3c6-2d2977f4be06",
   "metadata": {},
   "source": [
    "# Setup the LLM and RAG retrieval chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0468e9c5-7c1f-492d-8814-9acdd39e50c0",
   "metadata": {},
   "source": [
    "Construct a chain that can be used to generate a response based on a set of documents and a user query.\n",
    "\n",
    "Then construct a RAG retrieval chain that will take a user query as input, pass this information to the retriever to fetch relevant documents, and finally pass both the user query and the document content as context to the OpenAI model to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44516cc9-4cfc-40df-894f-0e5329ae68e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mBad file descriptor (C:\\Users\\runneradmin\\AppData\\Local\\Temp\\tmpkywtyyea\\build\\_deps\\bundled_libzmq-src\\src\\epoll.cpp:73). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbeb0f0-a146-4707-b878-c13773c69205",
   "metadata": {},
   "source": [
    "Construct the chain that will be subsequently used to generate a response based on a set of documents and a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50420a99-6595-46ec-a620-f7f8d95a9309",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mBad file descriptor (C:\\Users\\runneradmin\\AppData\\Local\\Temp\\tmpkywtyyea\\build\\_deps\\bundled_libzmq-src\\src\\epoll.cpp:73). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "    llm, prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d94fa1-5987-4e87-9730-f797c7b3af80",
   "metadata": {},
   "source": [
    "Create the retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56785d35-5234-4bd2-a669-e8b73a49b402",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mBad file descriptor (C:\\Users\\runneradmin\\AppData\\Local\\Temp\\tmpkywtyyea\\build\\_deps\\bundled_libzmq-src\\src\\epoll.cpp:73). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "rag_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "print(rag_chain.invoke({\"input\": \"Who is the CEO of the company?\"}).get(\"answer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef890d6c-3882-462b-a002-0c554ead663b",
   "metadata": {},
   "source": [
    "# Test the RAG Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a291d-e3a7-4cb6-a4c8-17bf083a2253",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mBad file descriptor (C:\\Users\\runneradmin\\AppData\\Local\\Temp\\tmpkywtyyea\\build\\_deps\\bundled_libzmq-src\\src\\epoll.cpp:73). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "input = \"What was the total revenue in Q2 2024?\"\n",
    "print(input)\n",
    "print(rag_chain.invoke({\"input\": input}).get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d55cc46-d310-4849-bea6-8328c4743ebf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mBad file descriptor (C:\\Users\\runneradmin\\AppData\\Local\\Temp\\tmpkywtyyea\\build\\_deps\\bundled_libzmq-src\\src\\epoll.cpp:73). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "input = \"Which department had the higest revenue?\"\n",
    "print(input)\n",
    "print(rag_chain.invoke({\"input\": input}).get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab80f3-6e0c-4ad2-992b-43e8dd42ba19",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mBad file descriptor (C:\\Users\\runneradmin\\AppData\\Local\\Temp\\tmpkywtyyea\\build\\_deps\\bundled_libzmq-src\\src\\epoll.cpp:73). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke({\"input\": input}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa1106-65d3-4b78-98ff-006487a79b5b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
